{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b34b4574a500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "from random import choices\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        #dimension of Gridworld \n",
    "        self.shape = (6,6)\n",
    "        #locations of the obstacles in the grid\n",
    "        self.obstacle_locs = [(1,1),(2,3),(2,5),(3,1),(4,1),(4,2),(4,4)]\n",
    "        #locations of the absorbing states\n",
    "        self.absorbing_locs = [(1,2),(4,3)]\n",
    "        #specific rewards respective to each terminal state \n",
    "        self.special_rewards = [10, -100]\n",
    "        #rewards for all other states\n",
    "        self.default_reward = -1\n",
    "        # Starting location\n",
    "        while True:\n",
    "            self.starting_loc = (np.random.randint(6),np.random.randint(6))\n",
    "            if (self.starting_loc in self.obstacle_locs)==False and (self.starting_loc in self.absorbing_locs)==False:\n",
    "                break\n",
    "        #names of the four actions possible\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        #number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        #defines probability of the desired direction\n",
    "        self.p = 0.45\n",
    "        #computes probability of the three other directions (all equal)\n",
    "        self.r = ((1 - self.p) / 3)\n",
    "        #randomizing action results: [1 0 0 0] to no Noise in the action results\n",
    "        self.action_randomizing_array = [self.p, self.r, self.r, self.r]\n",
    "        \n",
    "        ############################################\n",
    "        \n",
    "        ### Internal State ###\n",
    "        \n",
    "        #define attributes of the grid\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        #Number of valid states in the gridworld (there are 11 of them)\\n\",\n",
    "        self.state_size = state_size\n",
    "        #transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        #reward function (3D tensor)\n",
    "        self.R = R\n",
    "        #absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        #locations of valid states\n",
    "        self.locs = locs\n",
    "        #defines location of starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        #initialise starting state \n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        #defines walls of the grid \n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1 \n",
    "        #defines abosrbers of the grid \n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        #defines rewarders of the grid\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        #illustrate maps \n",
    "        self.paint_maps()\n",
    "        ############################################\n",
    "        \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    #computes transition matrix \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    #computes reward matrix\n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    ###########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    def value_iteration(self, discount = 0.2, threshold = 0.0001):\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        #Initialisation\n",
    "        epochs = 0\n",
    "        #initialise delta before increasing\n",
    "        delta = threshold\n",
    "        #define each value state at 0 for all states \n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        while delta >= threshold:\n",
    "            #increment epoch \n",
    "            epochs += 1 \n",
    "            #reinitialise delta value\n",
    "            delta = 0\n",
    "            \n",
    "            #considers each state \n",
    "            for state_idx in range(self.state_size):\n",
    "                \n",
    "                #IF not an absorbing state \n",
    "                if not(self.absorbing[0, state_idx]):  \n",
    "                \n",
    "                    #stores value computed at the previous state\n",
    "                    v = V[state_idx]\n",
    "                \n",
    "                    #computes Q value\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                        \n",
    "                    #set the new state value to the maximum of Q\n",
    "                    V[state_idx]= np.max(Q)\n",
    "                \n",
    "                    #compute new delta\n",
    "                    delta = max(delta, np.abs(v - V[state_idx]))\n",
    "                \n",
    "        #once loop is finished - fill in optimal policy \n",
    "        #initialise optimal policy\n",
    "        optimal_policy = np.zeros((self.state_size, selft.action_size))\n",
    "        \n",
    "        #considers each state \n",
    "        for state_idx in range(self.state_size):\n",
    "            \n",
    "            #computes Q value \n",
    "            Q = np.zeros(4)\n",
    "            #for successor state\n",
    "            for state_idx_prime in range(self.state_size):\n",
    "                Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "\n",
    "            #action that maximises Q values gets probability of 1 \n",
    "            optimal_policy[state_idx, np.argmax(Q)] = 1\n",
    "            \n",
    "        return optimal_policy, epochs\n",
    "    \n",
    "    def policy_iteration(self, discount = 0.2, threshold = 0.0001):\n",
    "        \n",
    "        #get transition and reward matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        #initialisation of policy\n",
    "        #sets policy to a vector of 0 \n",
    "        policy = np.zeros((self.state_size, self.action_size)) \n",
    "        #asks policy to always choose an action\n",
    "        policy[:,0] = 1\n",
    "        epochs = 0\n",
    "        #sets condition to stop the main loop \n",
    "        policy_stable = False\n",
    "        \n",
    "        while not(policy_stable):\n",
    "            \n",
    "            #POLICY EVALUATION \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            #increment epoch \n",
    "            epochs += epochs_eval \n",
    "            #sets policy to the boolean True - IF policy unstable, will return false\n",
    "            policy_stable = True\n",
    "            \n",
    "            #POLICY ITERATION\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                #IF not an absorbing state \n",
    "                if not(self.absorbing[0, state_idx]):\n",
    "                    \n",
    "                    # stores old action\n",
    "                    old_action = np.argmax(policy[state_idx,:])\n",
    "                    #computes Q value\n",
    "                    #initialise Q value to 0 \n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                        \n",
    "                    #compute corresponding policy\n",
    "                    new_policy = np.zeros(4)\n",
    "                    #action that maximises the Q value gets probability of 1 \n",
    "                    new_policy[np.argmax(Q)] = 1\n",
    "                    #update policy\n",
    "                    policy[state_idx] = new_policy\n",
    "                    \n",
    "                    #check if policy has converged \n",
    "                    if old_action != np.argmax(policy[state_idx]):\n",
    "                        #stops main loop if policy unstable\n",
    "                        policy_stable = False\n",
    "                        \n",
    "        return V, policy, epochs\n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "    \n",
    "        #set a delta bigger than the threshold \n",
    "        delta= 2*threshold\n",
    "        #get transition and reward matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        #initialise the value at 0 \n",
    "        V = np.zeros(policy.shape[0])\n",
    "        #make a copy of the value array to hold while evaluation's updating\n",
    "        Vnew = np.copy(V)\n",
    "        epoch=0\n",
    "        #while value is being converged (not yet converged)\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                #If absorbing state - continue \n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                     continue  \n",
    "                #accumulator variable for the Value of a state - temporary value?\n",
    "                tmpV = 0 \n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    #accumulator variable for the State-Action value \n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx,action_idx] + discount* V[state_idx_prime])\n",
    "                        \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                \n",
    "                #updtate state value \n",
    "                Vnew[state_idx] = tmpV\n",
    "                #once all state values updated - update delta \n",
    "                delta =  max(abs(Vnew-V))\n",
    "                #save new value into the old one \n",
    "                V=np.copy(Vnew)\n",
    "            #return optimal value function (?)\n",
    "            return V, epoch\n",
    "        \n",
    "        \n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    #### TD METHOD ###\n",
    "    \n",
    "    def eps_greedy(self, eps, Q):\n",
    "       \n",
    "        # assign same value to all elements\n",
    "        policy = np.full((Q.shape[0],Q.shape[1]), eps/self.action_size)\n",
    "        for state_idx in range(policy.shape[0]):\n",
    "            policy[state_idx, np.argmax(Q[state_idx])] = 1-eps + eps/self.action_size\n",
    "           \n",
    "        return policy\n",
    "    \n",
    "    \n",
    "    def sarsa(self, n, eps, gamma, alpha):\n",
    "        ## Initialization ##\n",
    "        Q = np.zeros((self.state_size, self.action_size))\n",
    "        epsilon_decay = 0.99\n",
    "        rewards = np.zeros(n)\n",
    "       \n",
    "        # value function for estimation error\n",
    "        V = np.zeros(self.state_size)\n",
    "        V_track = np.zeros((n,self.state_size))\n",
    "       \n",
    "        for i in range(n):\n",
    "           \n",
    "            # initialize S\n",
    "            state_idx = randrange(self.state_size)\n",
    "            while self.absorbing[0,state_idx]:\n",
    "                state_idx = randrange(self.state_size)\n",
    "           \n",
    "            # choose A using policy derivated from Q\n",
    "            policy = self.eps_greedy(eps, Q)\n",
    "            action = self.get_action(policy[state_idx])\n",
    "           \n",
    "            # for each step of the episode\n",
    "            while not self.absorbing[0,state_idx]:\n",
    "               \n",
    "                # take A from S and observe S' and R\n",
    "                state_prime_idx = self.get_next_state(state_idx, action)\n",
    "                reward = self.R[state_prime_idx, state_idx, action]\n",
    "                rewards[i] = rewards[i] + reward\n",
    "               \n",
    "                # choose A' form S' with new policy\n",
    "                policy = self.eps_greedy(eps, Q)\n",
    "                action_prime = self.get_action(policy[state_prime_idx])\n",
    "               \n",
    "                # update Q\n",
    "                Q[state_idx,action] = Q[state_idx,action] + math.log(i+1)/(i+1)*(\n",
    "                    reward + gamma*Q[state_prime_idx,action_prime] - Q[state_idx,action])\n",
    "               \n",
    "                # estimate V\n",
    "                V[int(state_idx)] = Q[int(state_idx), np.argmax(policy[int(state_idx)])]\n",
    "               \n",
    "                action = action_prime\n",
    "                state_idx = state_prime_idx\n",
    "               \n",
    "            #save Values\n",
    "            V_track[i] = V\n",
    "           \n",
    "            # decay epsilon\n",
    "            eps = eps * epsilon_decay\n",
    "           \n",
    "        return policy, Q, rewards, V_track\n",
    "            \n",
    "\n",
    "    ########### Internal Drawing Functions #####################\n",
    "        \n",
    "    def draw_deterministic_policy(self,Policy):\n",
    "        #draw a deterministic policy - needs to be a np array of 11 values \n",
    "        plt.figure()\n",
    "        #create graph of the grid\n",
    "        plt.imshow(self.walls+self.rewarders+self.absorbers)\n",
    "         \n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            #defines list of arrows corresponding to each possible action\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] \n",
    "            action_arrow= arrows[action]\n",
    "            #compute value location on graph\n",
    "            location = self.locs[state]\n",
    "            #place it on graph          \n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def draw_value(self, Value):\n",
    "        plt.figure()\n",
    "        #creates graph of the grid\n",
    "        plt.imshow(self.walls+self.rewarders+self.absorbers)\n",
    "        for state, value in enumerate(Value):\n",
    "            if(self.absorbing[0,state]):\n",
    "               continue\n",
    "            #computes value location on graph\n",
    "            location = self.locs[state]\n",
    "            #places it on graph\n",
    "            plt.text(location[1], location[0], round(value,2), ha='center', va='center')\n",
    "        plt.show()\n",
    "  \n",
    "    \n",
    "    def draw_deterministic_policy_grid(self, Policy, title, n_columns, n_lines):\n",
    "            plt.figure(figsize=(20,8))\n",
    "            for subplot in range (len(Policy)):\n",
    "                ax = plt.subplot(n_columns, n_lines, subplot+1)\n",
    "                ax.imshow(self.walls+self.rewarders+self.absorbers)\n",
    "                for state, action in enumerate(Policy[subplot]):\n",
    "                    if(self.absorbing[0,state]):\n",
    "                        continue\n",
    "                    arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] \n",
    "                    action_arrow = arrows[action]\n",
    "                    location = self.locs[state]\n",
    "                    plt.text(location[1], location[0], action_arrow, ha='center',va='center')\n",
    "                ax.title.set_text(title[subplot])\n",
    "            plt.show()\n",
    "   \n",
    "      \n",
    "    def draw_value_grid(self, Value, title, n_columns, n_lines):\n",
    "        plt.figure(figsize=(20,8)) #NEED TO CHANGE VALUES??\n",
    "        #goes through all values            \n",
    "        for subplot in range (len(Value)):\n",
    "            #creates a subplot for each value computed  \n",
    "            ax = plt.subplot(n_columns, n_lines, subplot+1)\n",
    "            #creates graph of the grid\n",
    "            ax.imshow(self.walls+self.rewarders+self.absorbers)\n",
    "            for state, value in enumerate(Value[subplot]):\n",
    "                #IF absorbing state - dont plot value \n",
    "                if(self.absorbing[0,state]):\n",
    "                      continue\n",
    "                #computes value location on graph\n",
    "                location = self.locs[state]\n",
    "                #places on the graph      \n",
    "                plt.text(location[1], location[0], round(value,1), ha='center', va='center')\n",
    "            ax.title.set_text(title[subplot])\n",
    "        plt.show()\n",
    "                      \n",
    "   ##########################\n",
    "                      \n",
    "   ########### Internal Helper Functions #####################\n",
    "               \n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.title('Obstacles')\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.title('Absorbing States')\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.title('Reward States')              \n",
    "        plt.show()\n",
    "                      \n",
    "    def build_grid_world(self):\n",
    "        #get : locations of all valid states, the neighbours of each state (by state number), the absorbing states (array of zeros WITH 1 when absorbing state)\n",
    "        locations, neighbours, absorbing = self.get_topology()              \n",
    "        \n",
    "        #get number of states \n",
    "        S = len(locations)\n",
    "        \n",
    "        #initialise the transition matrix \n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                #randomise the outcome when taking an action       \n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "        \n",
    "                #fill transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                      post_state = neighbours[prior_state, outcome]\n",
    "                      post_state = int(post_state)\n",
    "                      T[post_state, prior_state,action] = T[post_state, prior_state,action]+prob\n",
    "        \n",
    "        #build reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i], locations)\n",
    "            R[post_state,:,:]=sr          \n",
    "        return S,T,R, absorbing, locations               \n",
    "                      \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1\n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                #get location of each state\n",
    "                loc = (i,j)\n",
    "                #append it to valid state locations (IF valid state - not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "\n",
    "                    #get array with neighbours of each state\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr', 'ea', 'so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "        \n",
    "        #translate neighbours lists from locations to states \n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                #find neighbour location \n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                      \n",
    "                #turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "                      \n",
    "                #insert the state number into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                      \n",
    "        #translate absorbing locations into absorbing state indices \n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state]=1\n",
    "        return locs, state_neighbours, absorbing\n",
    "        \n",
    "                      \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index that corresponds to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "    \n",
    "    def is_location(self,loc):\n",
    "        #IF in the grid: valid location - not an obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #look for valid neighbours\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        #defines the transitions 'movements'\n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)              \n",
    "        so = (i+1,j)              \n",
    "        we = (i, j-1)   \n",
    "        \n",
    "        #IF valid location - executes direction requested - otherwise rejects it\n",
    "        if (direction == 'nr' and self.is_location(nr)):\n",
    "            return nr \n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so \n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we \n",
    "        #IF no other alternative - return to the same location \n",
    "        else: \n",
    "            return loc\n",
    "    \n",
    "        ###########################################         \n",
    "          \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation of my Grid world:\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GridWorld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e0cabe049097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m###defines the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Representation of my Grid world:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mPolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridWorld' is not defined"
     ]
    }
   ],
   "source": [
    "###defines the grid\n",
    "print(\"Representation of my Grid world:\\n\")\n",
    "grid = GridWorld()\n",
    "Policy = np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-59bf4b40d266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mreward_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumber_of_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_of_episode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_track\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_episode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mreward_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward_4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c55a4f7857a4>\u001b[0m in \u001b[0;36msarsa\u001b[0;34m(self, n, eps, gamma, alpha)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;31m# choose A' form S' with new policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                 \u001b[0maction_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_prime_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c55a4f7857a4>\u001b[0m in \u001b[0;36meps_greedy\u001b[0;34m(self, eps, Q)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# assign same value to all elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstate_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0meps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \"\"\"\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TD learning curve \n",
    "\n",
    "number_of_iteration = 300\n",
    "number_of_episode=500\n",
    "array_episodes=np.arange(0,499)\n",
    "reward_array = np.zeros([number_of_iteration,number_of_episode])\n",
    "for i in range(number_of_iteration):\n",
    "    policy, Q, reward_4, V_track = grid.sarsa(number_of_episode,0.01,0.2,0.5)\n",
    "    reward_array[i,:]=reward_4 \n",
    "\n",
    "#observation: as we increase prob and gamma - becomes more accurate (same for numb of iteration)\n",
    "\n",
    "#part a \n",
    "rewards_avg_0 = reward_array.mean(axis=0)\n",
    "\n",
    "print(\"\\nNumber of episodes vs Rewards\\n\")\n",
    "plt.plot((array_episodes, rewards_avg),linewidth=0.1,color='r',label=\"average_reward\")\n",
    "plt.show()\n",
    "\n",
    "rewards_avg_1 = reward_array.mean(axis=0) - reward_array.std(axis=0)\n",
    "#print(rewards_avg_0) #1D \n",
    "rewards_avg_2 = reward_array.mean(axis=0) + reward_array.std(axis=0)\n",
    "\n",
    "\n",
    "#part b -std \n",
    "print(\"\\nNumber of episodes vs Rewards(-std)\\n\")\n",
    "plt.plot((array_episodes, rewards_avg_1),linewidth=0.1,color='r',label=\"-std\")\n",
    "plt.show()\n",
    "\n",
    "#part b +std \n",
    "print(\"\\nNumber of episodes vs Rewards(+std)\\n\")\n",
    "plt.plot((array_episodes, rewards_avg_2),linewidth=0.1,color='r',label=\"+std\")\n",
    "plt.show()\n",
    "\n",
    "#array_episodes=np.arange(500)\n",
    "\n",
    "#plt.plot(array_episodes, rewards_avg)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
