{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    # Function to initialise the agent\n",
    "    def __init__(self):\n",
    "        # Set the episode length\n",
    "        self.episode_length = 100\n",
    "        # Reset the total number of steps which the agent has taken\n",
    "        self.num_steps_taken = 0\n",
    "        # The state variable stores the latest state of the agent in the environment\n",
    "        self.state = None\n",
    "        # The action variable stores the latest action which the agent has applied to the environment\n",
    "        self.action = None\n",
    "\n",
    "    # Function to check whether the agent has reached the end of an episode\n",
    "    def has_finished_episode(self):\n",
    "        if self.num_steps_taken % self.episode_length == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Function to get the next action, using whatever method you like\n",
    "    def get_next_action(self, state):\n",
    "        # Here, the action is random, but you can change this\n",
    "        action = np.random.uniform(low=-0.01, high=0.01, size=2).astype(np.float32)\n",
    "        # Update the number of steps which the agent has taken\n",
    "        self.num_steps_taken += 1\n",
    "        # Store the state; this will be used later, when storing the transition\n",
    "        self.state = state\n",
    "        # Store the action; this will be used later, when storing the transition\n",
    "        self.action = action\n",
    "        return action\n",
    "\n",
    "    # Function to set the next state and distance, which resulted from applying action self.action at state self.state\n",
    "    def set_next_state_and_distance(self, next_state, distance_to_goal):\n",
    "        # Convert the distance to a reward\n",
    "        reward = 1 - distance_to_goal\n",
    "        # Create a transition\n",
    "        #here self.action == self.discrete_action\n",
    "        transition = (self.state, self.action, reward, next_state)\n",
    "        # Now you can do something with this transition ...\n",
    "        self.state = next_state\n",
    "        # Update the agent's reward for this episode\n",
    "        self.total_reward += reward\n",
    "        return transition\n",
    "\n",
    "    # Function to get the greedy action for a particular state\n",
    "    def get_greedy_action(self, state):\n",
    "        # Here, the greedy action is fixed, but you should change it so that it returns the action with the highest Q-value\n",
    "        action = np.array([0.02, 0.0], dtype=np.float32)\n",
    "        return action\n",
    "    \n",
    "class DQN:\n",
    "    \n",
    "    # The class initialisation function.\n",
    "    def __init__(self):\n",
    "        # Create a Q-network, which predicts the q-value for a particular state.\n",
    "        self.q_network = Network(input_dimension=2, output_dimension=4)\n",
    "        # Define the optimiser which is used when updating the Q-network. \n",
    "        # The learning rate determines how big each gradient step is during backpropagation.\n",
    "        self.optimiser = torch.optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "    \n",
    "    def train_q_network(self, transition):\n",
    "        # Set all the gradients stored in the optimiser to zero.\n",
    "        self.optimiser.zero_grad()\n",
    "        # Calculate the loss for this transition.\n",
    "        loss = self._calculate_loss(transition)\n",
    "        # Compute the gradients based on this loss, i.e. the gradients of the loss with respect to the Q-network parameters.\n",
    "        loss.backward()\n",
    "        # Take one gradient step to update the Q-network.\n",
    "        self.optimiser.step()\n",
    "        # Return the loss as a scalar\n",
    "        return loss.item()\n",
    "    \n",
    "    \n",
    "    def _calculate_loss(self, transition):\n",
    "        #calculate the Q-network's loss based on its transition \n",
    "        #create state tensor \n",
    "        #.unsqueeze(0) - remove dimension // .squeeze() - add dimension\n",
    "        state_tensor = torch.tensor(transition[0], dtype=torch.float32)\n",
    "        #predict immediate reward \n",
    "        #here :online learning\n",
    "        predicted_reward_tensor = self.q_network.forward(state_tensor)[transition[1]]\n",
    "        \n",
    "        actual_immediate_reward_tensor = torch.tensor(transition[2], dtype=torch.float32)\n",
    "        \n",
    "        loss = torch.nn.MSE()(predicted_reward_tensor, actual_immediate_reward_tensor)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "#used to execute some code only if the file was run directly, and not imported\n",
    "if __name__ == \"main\":\n",
    "    \n",
    "    environment = Environment(display=True, magnification=500)\n",
    "    agent = Agent(environment)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
